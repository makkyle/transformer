{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a75963-77d4-4297-9ed7-7dac18a84f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## task 1) plot the distribution of the \"Binds to\" column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f26c59-5edb-46ad-8fd6-425de084f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub(x): \n",
    "    normal = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+-=()\"\n",
    "    sub_s = \"ₐ₈CDₑբGₕᵢⱼₖₗₘₙₒₚQᵣₛₜᵤᵥwₓᵧZₐ♭꜀ᑯₑբ₉ₕᵢⱼₖₗₘₙₒₚ૧ᵣₛₜᵤᵥwₓᵧ₂₀₁₂₃₄₅₆₇₈₉₊₋₌₍₎\"\n",
    "    res = x.maketrans(''.join(normal), ''.join(sub_s)) \n",
    "    return x.translate(res) \n",
    "\n",
    "ten_sub = get_sub('10')\n",
    "print(ten_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838201ee-a080-4261-8911-20a8f3cfa8fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Binds to\n",
    "# SARS-CoV-2                                                                               6918\n",
    "# SARS-CoV-2,SARS-CoV                                                                       521\n",
    "# SARS-CoV,SARS-CoV-2                                                                       387\n",
    "# SARS-CoV                                                                                   42\n",
    "# 229E,HKU1,NL63,SARS-CoV,SARS-CoV-2                                                         33\n",
    "# HKU1,SARS-CoV,SARS-CoV-2                                                                   29\n",
    "# SARS-CoV-2,SARS-CoV,OV43,HKU1                                                              23\n",
    "# SARS-CoV-2,HKU1                                                                            11\n",
    "# SARS-CoV-2,OC43                                                                             9\n",
    "# SARS-CoV-2,OC43,HKU1                                                                        8\n",
    "# HKU1                                                                                        7\n",
    "# SARS-CoV-2,SARS-CoV,HKU1                                                                    6\n",
    "# SARS-CoV-2,SARS-CoV,MERS-CoV,OC43,HKU1                                                      6\n",
    "# SARS-CoV,SARS-CoV-2,MERS-CoV,OC43                                                           5\n",
    "# 229E,HKU1,SARS-CoV,SARS-CoV-2                                                               5\n",
    "# SARS-CoV-2,SARS-CoV,OC43,HKU1                                                               5\n",
    "# SARS-CoV-2,MERS-CoV                                                                         4\n",
    "# SARS-CoV,SARS-CoV-2,229E,NL63,HKU1,OC43                                                     4\n",
    "# HKU1,NL63,SARS-CoV,SARS-CoV-2                                                               4\n",
    "# SARS-CoV,SARS-CoV-2,MERS-CoV,229E,NL63,HKU1,OC43                                            3\n",
    "# OC43,SARS-CoV,SARS-CoV-2                                                                    2\n",
    "# SARS-CoV,SARS-CoV-2,HKU1,OC43                                                               2\n",
    "# HKU1,SARS-CoV-2                                                                             2\n",
    "# SARS-CoV-2,SARS-CoV,OV43                                                                    2\n",
    "# OC43,HKU1                                                                                   2\n",
    "# SARS-CoV,SARS-CoV-2,MERS-CoV                                                                1\n",
    "# SARS-CoV,SARS-CoV-2,MERS-CoV,229E,NL63,HKU1,OC43,PCoV GXP4L,BCoV RsSHC014,BCoV RaTG13       1\n",
    "# 229E,SARS-CoV,SARS-CoV-2                                                                    1\n",
    "# SARS-CoV,SARS-CoV-2,MERS-CoV,NL63                                                           1\n",
    "# SARS-CoV,SARS-CoV-2,OC43                                                                    1\n",
    "# 229E,229E,HKU1,NL63,OC43,SARS-CoV,SARS-CoV-2                                                1\n",
    "# 229E,HKU1,SARS-CoV-2                                                                        1\n",
    "# SARS-CoV-2,HKU                                                                              1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_excel('CM_d1.xlsx', sheet_name='Table S1')\n",
    "df = df[[\"Patient ID\",\"Binds to\", \"Protein + Epitope\",\"CDRH3\", \"CDRL3\"]]\n",
    "# print(df[\"Binds to\"].value_counts())\n",
    "\n",
    "distr_data = df['Binds to'].value_counts()\n",
    "# print(type(distr_data))\n",
    "# print(distr_data)\n",
    "\n",
    "## convert the series into a dataframe \n",
    "df_distr_data = pd.DataFrame(distr_data)\n",
    "df_distr_data = df_distr_data.reset_index()\n",
    "# print(type(df_distr_data))\n",
    "# print(df_distr_data.info())\n",
    "# print(df_distr_data['Binds to'])\n",
    "\n",
    "## log10 of the count column \n",
    "df_distr_data['count'] = np.log10(df_distr_data['count'])\n",
    "df_distr_data.rename(columns={'count':'Frequency (log₁₀)'}, inplace=True)\n",
    "df_distr_data = df_distr_data.reset_index()\n",
    "print(df_distr_data.info())\n",
    "# df_distr_data.plot.scatter(x='index', y='Frequency (log10)')\n",
    "# plot = sns.scatterplot(data=df_distr_data, x=\"index\", y=\"Frequency (log10)\")\n",
    "# print(plot)\n",
    "\n",
    "plt.figure(figsize=(8, 6)) # Width and Height of the chart\n",
    "sns.lineplot(x='index',\n",
    "             y='Frequency (log₁₀)',\n",
    "             data=df_distr_data,\n",
    "             marker='o', # Style used to mark the join between 2 points\n",
    "            )\n",
    "plt.xlabel('Class sets', fontsize=30) # x-axis name\n",
    "plt.ylabel('Frequency (log₁₀)', fontsize=30) # y-axis name\n",
    "plt.title('Dataset data distribution', fontsize=30) # Add a title\n",
    "# plt.show() # Display the graph\n",
    "plt.savefig(\"scatterplot_data_distr.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74962ed2-5147-45bb-b3b4-86f835f860ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2870d3d3-4326-4243-8851-55c3cbbfd1e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harol\\AppData\\Local\\Temp\\ipykernel_37272\\2084501116.py:26: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Index([2735, 2736, 2737, 2738, 2739, 2841, 2842, 2843, 2845, 2846, 2847, 2848,\n",
      "       3153, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165,\n",
      "       3166, 3167, 3174, 3175, 3176, 3825, 4529, 5211, 5212, 5214, 5215, 5216,\n",
      "       5220, 5222, 5223, 5224, 7389, 7390],\n",
      "      dtype='int64'), Index([7260, 7265, 7270, 7302, 7303, 7304, 7305, 7306, 7307, 7308, 7309, 7310,\n",
      "       7311, 7312, 7315, 7318, 7322, 7329, 7330, 7331, 7332, 7334, 7335, 7336,\n",
      "       7337, 7338, 7339, 7340, 7341, 7342, 7343, 7344, 7345],\n",
      "      dtype='int64'), Index([7272, 7280, 7281, 7314, 7316, 7319, 7323, 7325, 7326, 7327, 7328, 7347,\n",
      "       7348, 7349, 7350, 7351, 7352, 7356, 7357, 7358, 7359, 7360, 7361, 7362,\n",
      "       7363, 7364, 7365, 7366, 7367],\n",
      "      dtype='int64'), Index([3814, 3819, 3897, 3903, 3907, 4159, 4320, 4331, 4545, 4549, 4552, 4563,\n",
      "       4601, 4668, 4694, 4716, 4855, 4927, 4930, 4942, 4955, 4964, 5084],\n",
      "      dtype='int64'), Index([3820, 3976, 3999, 4179, 4376, 4394, 4403, 4419, 4426, 4432, 4435], dtype='int64'), Index([8, 21, 2604, 4017, 4538, 4590, 4632, 4695, 5425], dtype='int64'), Index([3, 19, 27, 4082, 4190, 4532, 5427, 5430], dtype='int64'), Index([4202, 4206, 4221, 4427, 4664, 4685, 4815], dtype='int64')]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "## task 2) logo plot for each of these:\n",
    "# SARS-CoV                                                                                   42\n",
    "# HKU1                                                                                        7\n",
    "# HKU1,SARS-CoV,SARS-CoV-2                                                                   29\n",
    "# SARS-CoV-2,SARS-CoV,OV43,HKU1                                                              23\n",
    "# SARS-CoV-2,HKU1                                                                            11\n",
    "# SARS-CoV-2,OC43                                                                             9\n",
    "\n",
    "## new logoplots for clonotypes (generated in task 3)\n",
    "# 0 SARS-CoV                                                                                   42\n",
    "# 1 229E,HKU1,NL63,SARS-CoV,SARS-CoV-2                                                         33\n",
    "# 2 HKU1,SARS-CoV,SARS-CoV-2                                                                   29\n",
    "# 3 SARS-CoV-2,SARS-CoV,OV43,HKU1                                                              23\n",
    "# 4 SARS-CoV-2,HKU1                                                                            11\n",
    "# 5 SARS-CoV-2,OC43                                                                             9\n",
    "# 6 SARS-CoV-2,OC43,HKU1                                                                        8\n",
    "# 7 HKU1                                                                                        7\n",
    "\n",
    "# CL1 = 0 \n",
    "# CL2 = 1,2,3,4\n",
    "# CL3 = 5,6\n",
    "# CL4 = 7\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_excel('CM_d1.xlsx', sheet_name='Table S1')\n",
    "df = df[[\"Patient ID\",\"Binds to\", \"Protein + Epitope\",\"CDRH3\", \"CDRL3\"]]\n",
    "\n",
    "## step 1) locate the positions of the rows for each viral strain (listed in index_name)\n",
    "import random\n",
    "from random import sample \n",
    "index_list = []\n",
    "# index_name = ['SARS-CoV','HKU1', 'HKU1,SARS-CoV,SARS-CoV-2','SARS-CoV-2,SARS-CoV,OV43,HKU1','SARS-CoV-2,HKU1','SARS-CoV-2,OC43']\n",
    "index_name = ['SARS-CoV','229E,HKU1,NL63,SARS-CoV,SARS-CoV-2', 'HKU1,SARS-CoV,SARS-CoV-2','SARS-CoV-2,SARS-CoV,OV43,HKU1',\n",
    "              'SARS-CoV-2,HKU1','SARS-CoV-2,OC43', 'SARS-CoV-2,OC43,HKU1','HKU1']\n",
    "# index_name = ['SARS-CoV-2','SARS-CoV','229E', 'HKU1','OC43','NL63']\n",
    "\n",
    "for i in range(len(index_name)):\n",
    "    index_no = df.loc[(df == index_name[i]).any(axis=1)].index\n",
    "    # print(index_no)\n",
    "    index_list.append(index_no)\n",
    "print(index_list)\n",
    "print(len(index_list))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "721e6491-40c0-4e54-9a8e-46b2202b933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 21, 2604, 4017, 4538, 4590, 4632, 4695, 5425, 3, 19, 27, 4082, 4190, 4532, 5427, 5430]\n",
      "17\n",
      "1\n",
      "not appended\n",
      "2\n",
      "not appended\n",
      "3\n",
      "not appended\n",
      "4\n",
      "not appended\n",
      "5\n",
      "not appended\n",
      "6\n",
      "not appended\n",
      "7\n",
      "not appended\n",
      "8\n",
      "not appended\n",
      "9\n",
      "not appended\n",
      "10\n",
      "not appended\n",
      "11\n",
      "not appended\n",
      "12\n",
      "not appended\n",
      "13\n",
      "not appended\n",
      "14\n",
      "not appended\n",
      "15\n",
      "not appended\n",
      "16\n",
      "not appended\n",
      "17\n",
      "not appended\n",
      "18\n",
      "not appended\n",
      "19\n",
      "not appended\n",
      "20\n",
      "not appended\n",
      "21\n",
      "not appended\n",
      "22\n",
      "not appended\n",
      "23\n",
      "not appended\n",
      "24\n",
      "not appended\n",
      "25\n",
      "not appended\n",
      "26\n",
      "not appended\n",
      "27\n",
      "not appended\n",
      "28\n",
      "not appended\n",
      "29\n",
      "not appended\n",
      "30\n",
      "not appended\n",
      "31\n",
      "not appended\n",
      "32\n",
      "not appended\n",
      "33\n",
      "not appended\n",
      "34\n",
      "not appended\n",
      "35\n",
      "not appended\n",
      "36\n",
      "not appended\n",
      "37\n",
      "not appended\n",
      "38\n",
      "not appended\n",
      "39\n",
      "not appended\n",
      "40\n",
      "not appended\n",
      "41\n",
      "not appended\n",
      "42\n",
      "not appended\n",
      "43\n",
      "not appended\n",
      "44\n",
      "not appended\n",
      "45\n",
      "not appended\n",
      "46\n",
      "not appended\n",
      "47\n",
      "not appended\n",
      "48\n",
      "not appended\n",
      "49\n",
      "not appended\n",
      "50\n",
      "not appended\n",
      "51\n",
      "not appended\n",
      "52\n",
      "not appended\n",
      "53\n",
      "not appended\n",
      "54\n",
      "not appended\n",
      "55\n",
      "not appended\n",
      "56\n",
      "not appended\n",
      "57\n",
      "not appended\n",
      "58\n",
      "not appended\n",
      "59\n",
      "not appended\n",
      "60\n",
      "not appended\n",
      "61\n",
      "not appended\n",
      "62\n",
      "not appended\n",
      "63\n",
      "not appended\n",
      "64\n",
      "not appended\n",
      "65\n",
      "not appended\n",
      "66\n",
      "not appended\n",
      "67\n",
      "not appended\n",
      "68\n",
      "not appended\n",
      "69\n",
      "not appended\n",
      "70\n",
      "not appended\n",
      "71\n",
      "not appended\n",
      "72\n",
      "not appended\n",
      "73\n",
      "not appended\n",
      "74\n",
      "not appended\n",
      "75\n",
      "not appended\n",
      "76\n",
      "not appended\n",
      "77\n",
      "not appended\n",
      "78\n",
      "not appended\n",
      "79\n",
      "not appended\n",
      "80\n",
      "not appended\n",
      "81\n",
      "not appended\n",
      "82\n",
      "not appended\n",
      "83\n",
      "90.9090909090909\n",
      "appending to list\n",
      "ARVASTTGDDF ARVASTIGDDF\n",
      "84\n",
      "not appended\n",
      "85\n",
      "not appended\n",
      "86\n",
      "not appended\n",
      "87\n",
      "not appended\n",
      "88\n",
      "not appended\n",
      "89\n",
      "not appended\n",
      "90\n",
      "not appended\n",
      "91\n",
      "not appended\n",
      "92\n",
      "not appended\n",
      "93\n",
      "not appended\n",
      "94\n",
      "not appended\n",
      "95\n",
      "not appended\n",
      "96\n",
      "not appended\n",
      "97\n",
      "not appended\n",
      "98\n",
      "not appended\n",
      "99\n",
      "not appended\n",
      "100\n",
      "not appended\n",
      "101\n",
      "not appended\n",
      "102\n",
      "not appended\n",
      "103\n",
      "not appended\n",
      "104\n",
      "not appended\n",
      "105\n",
      "not appended\n",
      "106\n",
      "not appended\n",
      "107\n",
      "not appended\n",
      "108\n",
      "not appended\n",
      "109\n",
      "not appended\n",
      "110\n",
      "not appended\n",
      "111\n",
      "not appended\n",
      "112\n",
      "not appended\n",
      "113\n",
      "not appended\n",
      "114\n",
      "not appended\n",
      "115\n",
      "not appended\n",
      "116\n",
      "not appended\n",
      "117\n",
      "not appended\n",
      "118\n",
      "not appended\n",
      "119\n",
      "not appended\n",
      "120\n",
      "not appended\n",
      "121\n",
      "not appended\n",
      "122\n",
      "not appended\n",
      "123\n",
      "not appended\n",
      "124\n",
      "not appended\n",
      "125\n",
      "not appended\n",
      "126\n",
      "not appended\n",
      "127\n",
      "not appended\n",
      "128\n",
      "not appended\n",
      "129\n",
      "not appended\n",
      "130\n",
      "not appended\n",
      "131\n",
      "not appended\n",
      "132\n",
      "not appended\n",
      "133\n",
      "not appended\n",
      "134\n",
      "not appended\n",
      "135\n",
      "not appended\n",
      "136\n",
      "not appended\n",
      "137\n",
      "not appended\n",
      "138\n",
      "not appended\n",
      "139\n",
      "not appended\n",
      "140\n",
      "not appended\n",
      "141\n",
      "not appended\n",
      "142\n",
      "not appended\n",
      "143\n",
      "not appended\n",
      "144\n",
      "not appended\n",
      "145\n",
      "not appended\n",
      "146\n",
      "not appended\n",
      "147\n",
      "not appended\n",
      "148\n",
      "not appended\n",
      "149\n",
      "not appended\n",
      "150\n",
      "not appended\n",
      "151\n",
      "not appended\n",
      "152\n",
      "not appended\n",
      "153\n",
      "not appended\n",
      "154\n",
      "not appended\n",
      "155\n",
      "not appended\n",
      "156\n",
      "not appended\n",
      "157\n",
      "not appended\n",
      "158\n",
      "not appended\n",
      "159\n",
      "not appended\n",
      "160\n",
      "not appended\n",
      "161\n",
      "not appended\n",
      "162\n",
      "not appended\n",
      "163\n",
      "not appended\n",
      "164\n",
      "not appended\n",
      "165\n",
      "not appended\n",
      "166\n",
      "not appended\n",
      "167\n",
      "not appended\n",
      "168\n",
      "not appended\n",
      "169\n",
      "not appended\n",
      "170\n",
      "not appended\n",
      "171\n",
      "not appended\n",
      "172\n",
      "not appended\n",
      "173\n",
      "not appended\n",
      "174\n",
      "not appended\n",
      "175\n",
      "not appended\n",
      "176\n",
      "not appended\n",
      "177\n",
      "not appended\n",
      "178\n",
      "not appended\n",
      "179\n",
      "not appended\n",
      "180\n",
      "not appended\n",
      "181\n",
      "not appended\n",
      "182\n",
      "not appended\n",
      "183\n",
      "not appended\n",
      "184\n",
      "not appended\n",
      "185\n",
      "not appended\n",
      "186\n",
      "not appended\n",
      "187\n",
      "not appended\n",
      "188\n",
      "not appended\n",
      "189\n",
      "not appended\n",
      "190\n",
      "not appended\n",
      "191\n",
      "not appended\n",
      "192\n",
      "not appended\n",
      "193\n",
      "not appended\n",
      "194\n",
      "not appended\n",
      "195\n",
      "not appended\n",
      "196\n",
      "not appended\n",
      "197\n",
      "not appended\n",
      "198\n",
      "not appended\n",
      "199\n",
      "not appended\n",
      "200\n",
      "not appended\n",
      "201\n",
      "not appended\n",
      "202\n",
      "not appended\n",
      "203\n",
      "not appended\n",
      "204\n",
      "not appended\n",
      "205\n",
      "not appended\n",
      "206\n",
      "not appended\n",
      "207\n",
      "not appended\n",
      "208\n",
      "not appended\n",
      "209\n",
      "not appended\n",
      "210\n",
      "not appended\n",
      "211\n",
      "not appended\n",
      "212\n",
      "not appended\n",
      "213\n",
      "not appended\n",
      "214\n",
      "not appended\n",
      "215\n",
      "not appended\n",
      "216\n",
      "not appended\n",
      "217\n",
      "not appended\n",
      "218\n",
      "not appended\n",
      "219\n",
      "not appended\n",
      "220\n",
      "not appended\n",
      "221\n",
      "not appended\n",
      "222\n",
      "not appended\n",
      "223\n",
      "not appended\n",
      "224\n",
      "not appended\n",
      "225\n",
      "not appended\n",
      "226\n",
      "not appended\n",
      "227\n",
      "not appended\n",
      "228\n",
      "not appended\n",
      "229\n",
      "not appended\n",
      "230\n",
      "not appended\n",
      "231\n",
      "not appended\n",
      "232\n",
      "not appended\n",
      "233\n",
      "not appended\n",
      "234\n",
      "not appended\n",
      "235\n",
      "not appended\n",
      "236\n",
      "not appended\n",
      "237\n",
      "not appended\n",
      "238\n",
      "not appended\n",
      "239\n",
      "not appended\n",
      "240\n",
      "not appended\n",
      "241\n",
      "not appended\n",
      "242\n",
      "not appended\n",
      "243\n",
      "90.9090909090909\n",
      "appending to list\n",
      "ARVASTIGDDF ARVASTTGDDF\n",
      "244\n",
      "not appended\n",
      "245\n",
      "not appended\n",
      "246\n",
      "not appended\n",
      "247\n",
      "not appended\n",
      "248\n",
      "not appended\n",
      "249\n",
      "not appended\n",
      "250\n",
      "not appended\n",
      "251\n",
      "not appended\n",
      "252\n",
      "not appended\n",
      "253\n",
      "not appended\n",
      "254\n",
      "not appended\n",
      "255\n",
      "not appended\n",
      "256\n",
      "not appended\n",
      "257\n",
      "not appended\n",
      "258\n",
      "not appended\n",
      "259\n",
      "not appended\n",
      "260\n",
      "not appended\n",
      "261\n",
      "not appended\n",
      "262\n",
      "not appended\n",
      "263\n",
      "not appended\n",
      "264\n",
      "not appended\n",
      "265\n",
      "not appended\n",
      "266\n",
      "not appended\n",
      "267\n",
      "not appended\n",
      "268\n",
      "not appended\n",
      "269\n",
      "not appended\n",
      "270\n",
      "not appended\n",
      "271\n",
      "not appended\n",
      "272\n",
      "not appended\n",
      "273\n",
      "not appended\n",
      "274\n",
      "not appended\n",
      "275\n",
      "not appended\n",
      "276\n",
      "not appended\n",
      "277\n",
      "not appended\n",
      "278\n",
      "not appended\n",
      "279\n",
      "not appended\n",
      "280\n",
      "not appended\n",
      "281\n",
      "not appended\n",
      "282\n",
      "not appended\n",
      "283\n",
      "not appended\n",
      "284\n",
      "not appended\n",
      "285\n",
      "not appended\n",
      "286\n",
      "not appended\n",
      "287\n",
      "not appended\n",
      "288\n",
      "not appended\n",
      "289\n",
      "not appended\n",
      "['ARVASTTGDDF', 'ARVASTIGDDF', 'ARVASTIGDDF', 'ARVASTTGDDF']\n",
      "2\n",
      "ARVASTTGDDF\n",
      "11\n",
      "['ARVASTTGDDF', 'ARVASTIGDDF']\n"
     ]
    }
   ],
   "source": [
    "## step 2) calculate the 80% similarity between each sequences in each combination set \n",
    "import time\n",
    "import difflib\n",
    "\n",
    "CL1 = list(index_list[0])\n",
    "CL2 = list(index_list[1]) + list(index_list[2]) + list(index_list[3]) + list(index_list[4])  \n",
    "CL3 = list(index_list[5]) + list(index_list[6]) \n",
    "CL4 = list(index_list[7]) \n",
    "\n",
    "\n",
    "index_list_0 = CL3\n",
    "print(index_list_0)\n",
    "print(len(index_list_0))\n",
    "good_list = []\n",
    "index = 0 \n",
    "\n",
    "# def similarity_80_calculator(x,y):\n",
    "    \n",
    "        \n",
    "for i in range(len(index_list_0)):\n",
    "    for j in range(len(index_list_0)):\n",
    "        # print(index_list_0[i],index_list_0[j])\n",
    "        x = index_list_0[i]\n",
    "        y = index_list_0[j]\n",
    "        index += 1 \n",
    "        print(index)\n",
    "        \n",
    "        a = df[\"CDRH3\"][x]\n",
    "        b = df[\"CDRH3\"][y]\n",
    "        seq=difflib.SequenceMatcher(None,a ,b)\n",
    "        d=seq.ratio()*100\n",
    "        # d = int(d)\n",
    "        if d > 80 and d!=100:\n",
    "            good_list.append(a)\n",
    "            good_list.append(b)\n",
    "            print(d)\n",
    "            print(\"appending to list\")\n",
    "            print(a,b)\n",
    "        else:\n",
    "            print(\"not appended\")\n",
    "        # index += 1 \n",
    "        # print(index)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "print(good_list)\n",
    "\n",
    "\n",
    "\n",
    "# ## step 3a) remove duplicates from list\n",
    "good_list_dup_re = list(dict.fromkeys(good_list))\n",
    "print(len(good_list_dup_re))\n",
    "\n",
    "# good_list_index_dup_re = list(dict.fromkeys(good_list_index))\n",
    "# print(len(good_list_index_dup_re))\n",
    "# print(good_list_index_dup_re)\n",
    "\n",
    "mylist = good_list_dup_re\n",
    "\n",
    "\n",
    "### step 3b) introduce padding to solve the length inconsistency \n",
    "## find the longest sequence from that particular IGHV subgroup\n",
    "res = max(mylist, key = len)\n",
    "print(res)\n",
    "print(len(res))\n",
    "\n",
    "# ## introduce padding such that all sequences with same length \n",
    "for i in range(len(mylist)):\n",
    "    if len(mylist[i]) < len(res):\n",
    "        print(mylist[i])\n",
    "        print(\"length is:\" + str(len(mylist[i])))\n",
    "        add_index = len(res)-len(mylist[i])\n",
    "        print(add_index)\n",
    "        mylist[i] = mylist[i].ljust(add_index+len(mylist[i]),'-')\n",
    "        print(\"after change\")\n",
    "        print(len(mylist[i]))\n",
    "        # time.sleep(100)\n",
    "    else:\n",
    "        mylist[i] = mylist[i]\n",
    "        \n",
    "print(mylist)\n",
    "mylist_df = pd.DataFrame(mylist)\n",
    "mylist_df.to_csv(\"CL3_logoplot.csv\")\n",
    "\n",
    "\n",
    "# good_list.to_csv(\"SARS_1.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db50ab9e-644f-40b9-b77d-22e3010ec9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## task 3) heatmap generation \n",
    "\n",
    "\n",
    "\n",
    "## 8 clonotype classes \n",
    "# SARS-CoV                                                                                   42\n",
    "# 229E,HKU1,NL63,SARS-CoV,SARS-CoV-2                                                         33\n",
    "# HKU1,SARS-CoV,SARS-CoV-2                                                                   29\n",
    "# SARS-CoV-2,SARS-CoV,OV43,HKU1                                                              23\n",
    "# SARS-CoV-2,HKU1                                                                            11\n",
    "# SARS-CoV-2,OC43                                                                             9\n",
    "# SARS-CoV-2,OC43,HKU1                                                                        8\n",
    "# HKU1                                                                                        7\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_excel('CM_d1.xlsx', sheet_name='Table S1')\n",
    "df = df[[\"Patient ID\",\"Binds to\", \"Protein + Epitope\",\"CDRH3\", \"CDRL3\"]]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0e3ab-f1d5-4bb0-a19f-6abcea42952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['CDRH3'],inplace=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876eea0a-ec87-4c69-9455-7cef2062fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 1) locate all clonotype classes \n",
    "import random\n",
    "from random import sample \n",
    "index_list = []\n",
    "index_name = ['SARS-CoV','229E,HKU1,NL63,SARS-CoV,SARS-CoV-2', 'HKU1,SARS-CoV,SARS-CoV-2','SARS-CoV-2,SARS-CoV,OV43,HKU1',\n",
    "              'SARS-CoV-2,HKU1','SARS-CoV-2,OC43', 'SARS-CoV-2,OC43,HKU1','HKU1']\n",
    "# index_name = ['SARS-CoV-2','SARS-CoV','229E', 'HKU1','OC43','NL63']\n",
    "\n",
    "for i in range(len(index_name)):\n",
    "    index_no = df.loc[(df == index_name[i]).any(axis=1)].index\n",
    "    # print(index_no)\n",
    "    index_list.append(index_no)\n",
    "print(index_list)\n",
    "print(len(index_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b339471e-6ea2-4aad-8cbd-d316d62aa6dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 21, 2604, 4017, 4538, 4590, 4632, 4695, 5425, 3, 19, 27, 4082, 4190, 4532, 5427, 5430]\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "90.9090909090909\n",
      "appending to list\n",
      "ARVASTTGDDF ARVASTIGDDF\n",
      "4538 4532\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "90.9090909090909\n",
      "appending to list\n",
      "ARVASTIGDDF ARVASTTGDDF\n",
      "4532 4538\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "not appended\n",
      "count no of index to be added\n",
      "2\n",
      "good_list\n",
      "['ARVASTTGDDF', 'ARVASTIGDDF', 'ARVASTIGDDF', 'ARVASTTGDDF']\n",
      "4\n",
      "index_for_good_list\n",
      "[4538, 4532, 4532, 4538]\n",
      "4\n",
      "2\n",
      "2\n",
      "[4538, 4532]\n"
     ]
    }
   ],
   "source": [
    "## step 2) calculate the 80% similarity between each sequences in each combination set \n",
    "import time\n",
    "import difflib\n",
    "\n",
    "index_list_0 =  list(index_list[5]) + list(index_list[6]) \n",
    "print(index_list_0)\n",
    "good_list = []\n",
    "good_list_index = []\n",
    "index = 0 \n",
    "count_good_index = 0\n",
    "\n",
    "# # def similarity_80_calculator(x,y):\n",
    "    \n",
    "        \n",
    "for i in range(len(index_list_0)):\n",
    "    for j in range(len(index_list_0)):\n",
    "        # print(index_list_0[i],index_list_0[j])\n",
    "        # print(i,j)\n",
    "        x = index_list_0[i]\n",
    "        y = index_list_0[j]\n",
    "        \n",
    "        # index += 1 \n",
    "        # print(index)\n",
    "        # try:\n",
    "        a = df[\"CDRH3\"][x]\n",
    "        b = df[\"CDRH3\"][y]\n",
    "        seq=difflib.SequenceMatcher(None,a ,b)\n",
    "        d=seq.ratio()*100\n",
    "        # time.sleep(0.1)\n",
    "        # d = int(d)\n",
    "        if d > 80 and d!=100:\n",
    "            good_list.append(a)\n",
    "            good_list.append(b)\n",
    "            good_list_index.append(x)\n",
    "            good_list_index.append(y)\n",
    "            print(d)\n",
    "            print(\"appending to list\")\n",
    "            print(a,b)\n",
    "            print(x,y)\n",
    "            count_good_index += 1\n",
    "            \n",
    "        else:\n",
    "            print(\"not appended\")\n",
    "            # index += 1 \n",
    "            # print(index)\n",
    "        # # except:\n",
    "        # #     pass\n",
    "    \n",
    "print(\"count no of index to be added\")\n",
    "print(count_good_index)\n",
    "## obtain the good list of sequences with 80% similarity\n",
    "print('good_list')\n",
    "print(good_list)\n",
    "print(len(good_list))\n",
    "\n",
    "## obtain the index of all the sequences with 80% similarity\n",
    "print('index_for_good_list')\n",
    "print(good_list_index)\n",
    "print(len(good_list_index))\n",
    "\n",
    "## remove duplicates from list\n",
    "good_list_dup_re = list(dict.fromkeys(good_list))\n",
    "print(len(good_list_dup_re))\n",
    "\n",
    "good_list_index_dup_re = list(dict.fromkeys(good_list_index))\n",
    "print(len(good_list_index_dup_re))\n",
    "print(good_list_index_dup_re)\n",
    "\n",
    "good_list_index = good_list_index_dup_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d30de-983c-4c19-b8f7-8152dd045ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6488183f-676d-4661-bef4-9eb175150795",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2735\n",
    "y = 4427\n",
    "a = df[\"CDRH3\"][x]\n",
    "b = df[\"CDRH3\"][y]\n",
    "seq=difflib.SequenceMatcher(None,a ,b)\n",
    "d=seq.ratio()*100\n",
    "print(a,b)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ab7253c-1532-46cb-9a6e-92496caa42f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[4532]\n",
      "[3, 19, 27, 4082, 4190, 5427, 5430]\n"
     ]
    }
   ],
   "source": [
    "## check if the 2 combination sets have sequences with 80% similarity\n",
    "par_good_index_list = []\n",
    "par_bad_index_list = []\n",
    "number = 6\n",
    "for i in range(len(index_list[number])):\n",
    "    if index_list[number][i] in good_list_index_dup_re:\n",
    "        print('True')\n",
    "        par_good_index_list.append(index_list[number][i])\n",
    "    else:\n",
    "        par_bad_index_list.append(index_list[number][i])\n",
    "\n",
    "print(par_good_index_list)\n",
    "print(par_bad_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac3115-e0a6-41f3-93af-90c5d425c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = [0,0,0,0,0,0,0]\n",
    "viral_classes = ['SARS-CoV','SARS-CoV-2','229E','HKU1','NL63', 'OV43','OC43']\n",
    "for i in range(len(good_list_index)):\n",
    "    if viral_classes[0] in df['Binds to'][good_list_index[i]]:\n",
    "        class_count[0] += 1\n",
    "    if viral_classes[1] in df['Binds to'][good_list_index[i]]:\n",
    "        class_count[1] += 1\n",
    "    if viral_classes[2] in df['Binds to'][good_list_index[i]]:\n",
    "        class_count[2] += 1\n",
    "    if viral_classes[3] in df['Binds to'][good_list_index[i]]:\n",
    "        class_count[3] += 1\n",
    "    if viral_classes[4] in df['Binds to'][good_list_index[i]]:\n",
    "        class_count[4] += 1\n",
    "    if viral_classes[5] in df['Binds to'][good_list_index[i]]:\n",
    "        class_count[5] += 1\n",
    "    if viral_classes[6] in df['Binds to'][good_list_index[i]]:\n",
    "        class_count[6] += 1\n",
    "\n",
    "print(class_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf05f3-4c01-46e3-ac9a-66a4d152dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "myInt = sum(class_count)\n",
    "print(myInt)\n",
    "class_count = [x / myInt for x in class_count]\n",
    "print(class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b786b96-fc12-495d-aaeb-1ededaedae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CL_score = [92, 0, 0, 0, 0, 0, 0, 126, 126, 126, 126, 126, 0, 0, 78, 78, 0, 78, 0, 0, 0]\n",
    "# CL_score = []\n",
    "CL_score = CL_score + class_count \n",
    "print(CL_score)\n",
    "print(len(CL_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5562c3-8913-41b5-aade-0219b051a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CL_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3bdbb9-1980-4791-b990-a364be6862e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate the dataframe for heatmap generation\n",
    "\n",
    "synthetic_df = pd.DataFrame(columns=['clonotype(CL)','viral_strain','>80%_similarity'])\n",
    "CL1 = [\"CL1\"]\n",
    "CL2 = [\"CL2\"]\n",
    "CL3 = [\"CL3\"]\n",
    "CL4 = [\"CL4\"]\n",
    "CL5 = [\"CL5\"]\n",
    "CL6 = [\"CL6\"]\n",
    "CL7 = [\"CL7\"]\n",
    "CL8 = [\"CL8\"]\n",
    "viral_classes = ['SARS-CoV','SARS-CoV-2','229E','HKU1','NL63', 'OV43','OC43']\n",
    "\n",
    "n = 7\n",
    "\n",
    "CL = CL1*n + CL2*n + CL3*n + CL4*n\n",
    "# + CL5*n + CL6*n + CL7*n + CL8*n\n",
    "print(CL)\n",
    "\n",
    "viral_classes_list = viral_classes*4\n",
    "\n",
    "synthetic_df['clonotype(CL)'] = CL\n",
    "synthetic_df['viral_strain'] = viral_classes_list\n",
    "synthetic_df['>80%_similarity'] = CL_score\n",
    "\n",
    "print(synthetic_df)\n",
    "\n",
    "\n",
    "# synthetic_df.to_csv(\"example_df_for_demo.csv\")\n",
    "## if clonotype and viral_strain both have single entry and also same name\n",
    "## >80%_similarity column is 100%\n",
    "\n",
    "synthetic_df = synthetic_df.fillna(0)\n",
    "\n",
    "# for i in range(len(synthetic_df)):\n",
    "#     if synthetic_df['clonotype(CL)'][i] == synthetic_df['viral_strain'][i]:\n",
    "#         synthetic_df['>80%_similarity'][i] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "##plot\n",
    "# pivot the dataframe from long to wide form\n",
    "result = synthetic_df.pivot(index='clonotype(CL)', columns='viral_strain', values='>80%_similarity')\n",
    "\n",
    "sns.heatmap(result, annot=True, cmap='viridis',square=True)\n",
    "plt.savefig('heatmap_new.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8998b97f-18b2-4ab3-a658-36fcf8a39e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CL_score[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac25e0b-8331-441c-bb4e-d0a7063fec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "CL_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721270e-b4f8-4266-acf6-d9eaeb30bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Reds')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674abd37-1510-4e5a-8a2c-c2e540d5d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_confusion_matrix(cm = np.array([[ 10980,  1000],\n",
    "                                              [  1620,  3464]]), \n",
    "                      normalize    = False,\n",
    "                      target_names = ['Positive', 'Negative'],\n",
    "                      title        = \"Confusion Matrix\")\n",
    "\n",
    "plt.savefig('confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4539081c-9a79-4563-9fa0-ae79b6044dce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
